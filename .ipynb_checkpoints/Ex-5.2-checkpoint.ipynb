{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geospatial Data Analysis\n",
    "## Excercise 5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input features import and reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial shape:  (145, 145, 200)\n",
      "Final shape:  (21025, 200)\n"
     ]
    }
   ],
   "source": [
    "X = np.load(os.path.join(\"data\", \"partB\", \"indianpinearray.npy\"))\n",
    "print(\"Initial shape: \", X.shape)\n",
    "X = X.reshape(-1,200)\n",
    "print(\"Final shape: \", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground truth import and reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial shape:  (145, 145)\n",
      "Final shape:  (21025,)\n",
      "Classes:  [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16]\n"
     ]
    }
   ],
   "source": [
    "y = np.load(os.path.join(\"data\", \"partB\", \"IPgt.npy\"))\n",
    "print(\"Initial shape: \", y.shape)\n",
    "y = y.reshape(-1)\n",
    "print(\"Final shape: \", y.shape)\n",
    "print(\"Classes: \", np.unique(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also class 0 in the dataset. I remove class 0 which corresponds to unlabeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final shapes:  (10249, 200) (10249,)\n",
      "Classes:  [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16]\n"
     ]
    }
   ],
   "source": [
    "X = X[y!=0]\n",
    "y = y[y!=0]\n",
    "print(\"Final shapes: \", X.shape, y.shape)\n",
    "print(\"Classes: \", np.unique(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I normalise X inside [0,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "sc = preprocessing.MinMaxScaler()\n",
    "X = sc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first we train the classifiers using the GridSearch algorithm so as to decide the best hyperparameter values based on accuracy. We try different hyperparameter values for both kernel SVM and RandomForest algorithms. I also split in training/ test set, equally balancing all classes and I use less samples for tuning given that especially the RF algorithm is really resource consuming. We also use stratified K-Fold in GridSearch so as to be sure that cross validation is performed with balanced splits using StratifiedKFold CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning hyper-parameters, based on accuracy for: SVM\n",
      "with parameter choice:\n",
      "{'kernel': ['rbf', 'linear'], 'gamma': ['scale', 'auto'], 'C': [10, 100, 1000]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train / Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=69, stratify=y)\n",
    "#Set the parameters of each model by cross-validation gridsearch\n",
    "\n",
    "names = [\"SVM\", \"RandomForest\"]\n",
    "algorithms = [SVC(), RandomForestClassifier(n_jobs=-1)]\n",
    "tuned_parameters = [{'kernel': ['rbf', 'linear'], 'gamma': ['scale', 'auto'], 'C': [10, 100, 1000]},\n",
    "                    {'n_estimators': [200, 600], 'max_depth': [4, 10, None], 'min_samples_leaf': [1, 2, 5]}]\n",
    "\n",
    "best_scores = []\n",
    "params = []\n",
    "kfolds = StratifiedKFold(3)\n",
    "# Gridsearch loop for all classifiers\n",
    "i=0\n",
    "for (a, t_p) in list(zip(algorithms, tuned_parameters)):\n",
    "    print(\"Tuning hyper-parameters, based on accuracy for: {}\\nwith parameter choice:\\n{}\\n\".format(names[i], t_p))\n",
    "    clf = GridSearchCV(a, t_p, cv=kfolds.split(X_train, y_train), scoring = 'accuracy', n_jobs=-1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(\"Mean performance of each parameter combination based on Cross Validation\")\n",
    "    performance = pd.DataFrame(clf.cv_results_['params'])\n",
    "    performance[\"Score\"] = clf.cv_results_['mean_test_score']\n",
    "    print(performance)\n",
    "    print(\"Best parameters set found on training set:\")\n",
    "    print(clf.best_params_)\n",
    "    print(\"\\nThe scores are computed on the full evaluation set for the best combination of parameters\")\n",
    "    #evaluate and store scores of estimators of each category on test set\n",
    "    score = clf.score(X_test, y_test)\n",
    "    print(\"Test score:\", score)\n",
    "    print(\"============================\\n\")\n",
    "    #store round scores\n",
    "    cv_scores.append(clf.cv_results_[\"mean_test_score\"])\n",
    "    params.append(clf.best_params_)\n",
    "    best_scores.append(score)\n",
    "    i+=1\n",
    "final_scores = dict(zip(names, best_scores))\n",
    "print(final_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# NumPy to Tensors\n",
    "# =============================================================================\n",
    "# Create tensors from numpy arrays\n",
    "input = torch.Tensor(X_train).view(len(X_train),-1, 1)\n",
    "target = torch.Tensor(y_train).view(len(X_train),-1, 1)\n",
    "input_test = torch.Tensor(X_test).view(len(X_test),-1, 1)\n",
    "target_test = torch.Tensor(y_test).view(len(y_test),-1, 1)\n",
    "\n",
    "# =============================================================================\n",
    "# Cuda\n",
    "# =============================================================================\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "    input = input.cuda()\n",
    "    target = target.cuda()\n",
    "    input_test = input_test.cuda()\n",
    "    target_test = target_test.cuda()\n",
    "else:\n",
    "    print(\"GPU not available, CPU used\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Train and Evaluate the model\n",
    "# =============================================================================\n",
    "epochs = 200\n",
    "epoch = 0\n",
    "epoch_loss_val = []\n",
    "epoch_loss_train = []\n",
    "patience = 50 # initialise here\n",
    "countdown = patience # Early stopping counter (epochs to wait)\n",
    "while epoch < epochs and countdown > 0:\n",
    "    # Training\n",
    "    epoch +=1\n",
    "    batch_loss = []\n",
    "    # enumerate brings a batch of the data.\n",
    "    for i, data in enumerate(trainloader):\n",
    "        inputs = data['timeseries'].to(device) \n",
    "        labels, lengths = data['labels'].to(device), data['lengths'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        output = model(inputs, lengths)\n",
    "        loss = criterion(output, labels)\n",
    "        batch_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"epoch: %d, loss: %1.5f\" % (epoch, np.mean(batch_loss)))\n",
    "    epoch_loss_train.append(np.mean(batch_loss))\n",
    "    # Validation\n",
    "    batch_loss = []\n",
    "    batch_acc = []\n",
    "    y_pred_val = []\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(valloader):\n",
    "            inputs_val = data['timeseries'].to(device) \n",
    "            labels_val = data['labels'].to(device)\n",
    "            lengths_val = data['lengths'].to(device)\n",
    "            batch_pred = model(inputs_val, lengths_val)\n",
    "            loss = criterion(batch_pred, labels_val)\n",
    "            batch_loss.append(loss.item())\n",
    "            batch_pred = [np.argmax(batch_pred[i].\n",
    "                              to(torch.device('cpu')).\n",
    "                              detach().numpy()) \n",
    "                          for i in range(len(batch_pred))]\n",
    "            y_pred_val.append(batch_pred)\n",
    "            batch_acc.append(metrics.\n",
    "                             accuracy_score(labels_val.\n",
    "                                            to(torch.device('cpu')).\n",
    "                                            detach().numpy(), batch_pred))\n",
    "    epoch_loss_val.append(np.mean(batch_loss))\n",
    "    print(\"Validation loss: {:1.3f}, Validation Acc: {:1.3f}, Countdown: {} \\n\"\n",
    "          .format(epoch_loss_val[-1], np.mean(batch_acc), countdown))\n",
    "    # Early stopping condtion: N epochs without achieving loss than the\n",
    "    # present minimum. No need to save models before patience\n",
    "    if epoch_loss_val[-1] <= min(epoch_loss_val):\n",
    "        countdown = patience #start countdown\n",
    "    #checkpoint \n",
    "        if epoch >= patience: # no need to save before that\n",
    "    #I ovewrite models so as to keep the last to trigger the countdown\n",
    "            torch.save(model, os.path.join(os.getcwd(),\n",
    "                      \"models\"+os.path.sep+best_model_name+\".pt\"))\n",
    "    else:\n",
    "        countdown -= 1\n",
    "print(\"Finished Training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Plot Train / Validation Loss\n",
    "# =============================================================================\n",
    "plt.rcParams[\"figure.figsize\"] = (20,6)\n",
    "plt.figure()\n",
    "plt.title(\"Relative Loss\")\n",
    "plt.plot(list(range(1,epoch+1)), epoch_loss_train, label='Training set')\n",
    "plt.plot(list(range(1,epoch+1)), epoch_loss_val,  label='Validation set')\n",
    "plt.grid()\n",
    "plt.ylim(0, 0.5)\n",
    "plt.legend(fancybox=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Evaluation with various classification metrics (classification report)\n",
    "###############################################################################\n",
    "from confusion_matrix import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=69, stratify=y)\n",
    "\n",
    "models = {'kNN': KNeighborsClassifier(n_neighbors=3),\n",
    "          'NB': GaussianNB(),\n",
    "          'MLP1':\n",
    "          'MLP2':  }\n",
    "# Create scores dictionary for each algorithm\n",
    "scores=[]\n",
    "mdl=[]\n",
    "results=[]\n",
    "for model in models.keys():\n",
    "    clf = models[model]\n",
    "    clf.fit(X_train,  y_train)\n",
    "    mdl.append(model)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    results.append((clf.score(X_test, y_test), y_pred))\n",
    "    print (model, \"\\n\")\n",
    "    print(metrics.classification_report(y_test, y_pred, digits=5))\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cm, classes=[0,1], title=model, cmap=plt.cm.Greens)\n",
    "    print(\"True Positives: {}, False Positives: {}, True Negatives:, False Negatives: {} \\n\\n\".format(cm[0,0], cm[0,1], cm[1,1], cm[1,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Evaluation with various classification metrics (classification report)\n",
    "###############################################################################\n",
    "from confusion_matrix import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "N=1000\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=69, stratify=y)\n",
    "X_train, X_test, y_train, y_test = X_train[:N,:], X_test[:N,:], y_train[:N], y_test[:N]\n",
    "\n",
    "models = {'kNN': KNeighborsClassifier(n_neighbors=3),\n",
    "          'NB': GaussianNB(),\n",
    "          'Perceptron': Perceptron(penalty='l1')}\n",
    "# Create scores dictionary for each algorithm\n",
    "scores=[]\n",
    "mdl=[]\n",
    "results=[]\n",
    "for model in models.keys():\n",
    "    clf = models[model]\n",
    "    clf.fit(X_train,  y_train)\n",
    "    mdl.append(model)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    results.append((clf.score(X_test, y_test), y_pred))\n",
    "    print (model, \"\\n\")\n",
    "    print(metrics.classification_report(y_test, y_pred, digits=5))\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cm, classes=[0,1], title=model, cmap=plt.cm.Greens)\n",
    "    print(\"True Positives: {}, False Positives: {}, True Negatives:, False Negatives: {} \\n\\n\".format(cm[0,0], cm[0,1], cm[1,1], cm[1,0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
